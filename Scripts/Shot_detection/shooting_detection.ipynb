{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0156b772-4462-48f3-8f68-5f71acec6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dacf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the versions\n",
    "# opencv_version = cv2.__version__\n",
    "# ultralytics_version = ultralytics.__version__\n",
    "# mediapipe_version = mp.__version__\n",
    "\n",
    "# # Write the versions to a requirements.txt file\n",
    "# with open('/content/drive/MyDrive/bollogy/requirements.txt', 'w') as f:\n",
    "#     f.write(f\"opencv-python=={opencv_version}\\n\")\n",
    "#     f.write(f\"ultralytics=={ultralytics_version}\\n\")\n",
    "#     f.write(f\"mediapipe=={mediapipe_version}\\n\")\n",
    "#     f.close()\n",
    "\n",
    "# print(\"requirements.txt file has been created with the following content:\")\n",
    "# print(f\"opencv-python=={opencv_version}\")\n",
    "# print(f\"ultralytics=={ultralytics_version}\")\n",
    "# print(f\"mediapipe=={mediapipe_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66106dbe-fffb-47e1-9ba6-c2f925e3ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "model_path = 'yolov8n.pt'  # Using a pre-trained YOLOv8 model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, enable_segmentation=False, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b79e8e-6435-4038-8e6c-79342c5f8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save results\n",
    "bbox_output_dir = 'Output\\Detected_bbox'\n",
    "Handsup_output_dir = 'Output/Hands_Up/'\n",
    "xml_dir = 'Output/XML_Files'\n",
    "os.makedirs(Handsup_output_dir, exist_ok=True)\n",
    "os.makedirs(xml_dir, exist_ok=True)\n",
    "\n",
    "# Confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8108a5-0fe8-4d2d-aca5-35cad3efae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_landmarks(image):\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = [landmark for landmark in results.pose_landmarks.landmark if landmark.visibility > confidence_threshold]\n",
    "        if len(keypoints) > len(results.pose_landmarks.landmark) / 2:\n",
    "            return results.pose_landmarks\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cf6483-50e3-4424-b621-e9b3b6595d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pose(landmarks):\n",
    "    if not landmarks:\n",
    "        return \"No pose detected\"\n",
    "\n",
    "    landmarks_list = [landmark for landmark in landmarks.landmark]\n",
    "\n",
    "    left_wrist = landmarks_list[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "    right_wrist = landmarks_list[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "    left_eye = landmarks_list[mp_pose.PoseLandmark.LEFT_EYE.value]\n",
    "    right_eye = landmarks_list[mp_pose.PoseLandmark.RIGHT_EYE.value]\n",
    "    nose = landmarks_list[mp_pose.PoseLandmark.NOSE.value]\n",
    "\n",
    "    # Check if either wrist is above the eyes or nose\n",
    "    if (left_wrist.y < left_eye.y or left_wrist.y < nose.y) and (right_wrist.y < right_eye.y or right_wrist.y < nose.y):\n",
    "        return \"Hands Up\"\n",
    "    else:\n",
    "        return \"Other Pose\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec3169a-2552-4cd4-951f-5f399cb25a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xml_for_bounding_box(bounding_box, file_name, save_directory):\n",
    "    \"\"\"\n",
    "    Create an XML file for a given bounding box and file name, and save it to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    bounding_box (tuple): A tuple containing (xmin, ymin, xmax, ymax).\n",
    "    file_name (str): The name of the file (without extension).\n",
    "    save_directory (str): The directory where the XML file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    str: The name of the created XML file.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the root element\n",
    "    annotation = ET.Element('annotation')\n",
    "\n",
    "    # Define the folder element\n",
    "    folder = ET.SubElement(annotation, 'folder')\n",
    "    folder.text = 'images'\n",
    "\n",
    "    # Define the filename element\n",
    "    filename = ET.SubElement(annotation, 'filename')\n",
    "    filename.text = f\"{file_name}.jpg\"\n",
    "\n",
    "    # Define the path element\n",
    "    path = ET.SubElement(annotation, 'path')\n",
    "    path.text = os.path.join('Output\\XML_Files', f\"{file_name}.jpg\")\n",
    "\n",
    "    # Define the source element\n",
    "    source = ET.SubElement(annotation, 'source')\n",
    "    database = ET.SubElement(source, 'database')\n",
    "    database.text = 'Unknown'\n",
    "\n",
    "    # Define the size element\n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    width = ET.SubElement(size, 'width')\n",
    "    width.text = '1280'  # Placeholder value\n",
    "    height = ET.SubElement(size, 'height')\n",
    "    height.text = '720'  # Placeholder value\n",
    "    depth = ET.SubElement(size, 'depth')\n",
    "    depth.text = '3'  # Assuming RGB images\n",
    "\n",
    "    # Define the segmented element\n",
    "    segmented = ET.SubElement(annotation, 'segmented')\n",
    "    segmented.text = '0'\n",
    "\n",
    "    # Define the object element\n",
    "    obj = ET.SubElement(annotation, 'object')\n",
    "    name = ET.SubElement(obj, 'name')\n",
    "    name.text = 'shooting player'\n",
    "    pose = ET.SubElement(obj, 'pose')\n",
    "    pose.text = 'Unspecified'\n",
    "    truncated = ET.SubElement(obj, 'truncated')\n",
    "    truncated.text = '0'\n",
    "    difficult = ET.SubElement(obj, 'difficult')\n",
    "    difficult.text = '0'\n",
    "    bndbox = ET.SubElement(obj, 'bndbox')\n",
    "    xmin = ET.SubElement(bndbox, 'xmin')\n",
    "    xmin.text = str(int(bounding_box[0]))\n",
    "    ymin = ET.SubElement(bndbox, 'ymin')\n",
    "    ymin.text = str(int(bounding_box[1]))\n",
    "    xmax = ET.SubElement(bndbox, 'xmax')\n",
    "    xmax.text = str(int(bounding_box[2]))\n",
    "    ymax = ET.SubElement(bndbox, 'ymax')\n",
    "    ymax.text = str(int(bounding_box[3]))\n",
    "\n",
    "    # Convert the tree to a string and write it to a file\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    xml_file_name = os.path.join(save_directory, f\"{file_name}.xml\")\n",
    "    tree.write(xml_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9a3171-1ff6-4591-bbe5-f56aa032291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(model, frame):\n",
    "    results = model.predict(frame)\n",
    "    detected_persons = []\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        scores = result.boxes.conf.cpu().numpy()\n",
    "        class_ids = result.boxes.cls.cpu().numpy()\n",
    "\n",
    "        for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "            if class_id == 0 and score > confidence_threshold:  # Class 0 is 'person' in COCO dataset\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                height = y2 - y1\n",
    "                width = x2 - x1\n",
    "                center_x = (x1 + x2) // 2\n",
    "                x1 = center_x - height // 2\n",
    "                x2 = center_x + height // 2\n",
    "                x1 = max(0, x1)\n",
    "                x2 = min(frame.shape[1], x2)\n",
    "                cropped_person = frame[y1:y2, x1:x2]\n",
    "                landmarks = get_pose_landmarks(cropped_person)\n",
    "                pose_label = classify_pose(landmarks)\n",
    "                if pose_label == \"Hands Up\":\n",
    "                    print('Hands UP')\n",
    "                    detected_persons.append((box, score, class_id, pose_label))\n",
    "    return detected_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4218d7-1adb-492f-9000-8e4ba7f8e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_name(length=8):\n",
    "    \"\"\"Generate a random string of letters and digits.\"\"\"\n",
    "    chars = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(chars) for _ in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3eb9cf1-84f9-48ac-99fd-57863e8be449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     15\u001b[0m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m detected_persons \u001b[38;5;241m=\u001b[39m detect_person(model, frame)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, score, class_id, pose_label \u001b[38;5;129;01min\u001b[39;00m detected_persons:\n\u001b[0;32m     20\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_random_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36mdetect_person\u001b[1;34m(model, frame)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_person\u001b[39m(model, frame):\n\u001b[1;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(frame)\n\u001b[0;32m      3\u001b[0m     detected_persons \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:444\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\predict.py:25\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[1;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mnon_max_suppression(\n\u001b[0;32m     26\u001b[0m         preds,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mconf,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39miou,\n\u001b[0;32m     29\u001b[0m         agnostic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39magnostic_nms,\n\u001b[0;32m     30\u001b[0m         max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_det,\n\u001b[0;32m     31\u001b[0m         classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mclasses,\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\ops.py:291\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m c  \u001b[38;5;66;03m# boxes (offset by class)\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     i \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mnms(boxes, scores, iou_thres)  \u001b[38;5;66;03m# NMS\u001b[39;00m\n\u001b[0;32m    292\u001b[0m i \u001b[38;5;241m=\u001b[39m i[:max_det]  \u001b[38;5;66;03m# limit detections\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# # Experimental\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# merge = False  # use merge-NMS\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m#     if redundant:\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m#         i = i[iou.sum(1) > 1]  # require redundancy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\torchvision\\ops\\boxes.py:40\u001b[0m, in \u001b[0;36mnms\u001b[1;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[1;32m---> 40\u001b[0m _assert_has_ops()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision\u001b[38;5;241m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[1;32mc:\\Users\\alimo\\anaconda3\\Lib\\site-packages\\torchvision\\extension.py:48\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_has_ops\u001b[39m():\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_ops():\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "# Process each video\n",
    "video_dir = 'C:/Users/alimo/OneDrive/Documents/FRT/Ballogy Computer Vision/Ballogy-Computer-Vision/Videos_data'  # Update with video path\n",
    "frame_counter = 0\n",
    "count=0\n",
    "\n",
    "for video_file in os.listdir(video_dir):\n",
    "    if video_file.endswith('.mp4'):\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            count+=1\n",
    "            detected_persons = detect_person(model, frame)\n",
    "\n",
    "            for box, score, class_id, pose_label in detected_persons:\n",
    "                \n",
    "                file_name = f'{generate_random_name()}.jpg'\n",
    "                file_path = os.path.join(Handsup_output_dir,file_name)\n",
    "                cv2.imwrite(file_path, frame)\n",
    "                \n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                bbox_file_path = os.path.join(bbox_output_dir, file_name)\n",
    "                cv2.imwrite(bbox_file_path, image_rgb)\n",
    "\n",
    "                # create_pascal_voc_annotation(file_name, frame.shape, bounding_boxes, xml_dir)\n",
    "                create_xml_for_bounding_box(box, file_name.replace('.jpg', ''), xml_dir)\n",
    "                frame_counter += 1\n",
    "            print(count)\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01037425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.2a0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650de67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
